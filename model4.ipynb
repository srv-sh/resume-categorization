{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourav/miniconda3/envs/monty/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-12 15:29:18.117651: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-12 15:29:18.180840: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-12 15:29:18.183384: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-12 15:29:18.183392: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-08-12 15:29:18.194896: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-08-12 15:29:18.448995: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-12 15:29:18.449023: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-12 15:29:18.449025: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('data/dataset/train.csv')\n",
    "test_dataset = pd.read_csv('data/dataset/test.csv')\n",
    "val_dataset = pd.read_csv('data/dataset/val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.dropna(inplace = True)\n",
    "val_dataset.dropna(inplace = True)\n",
    "test_dataset.dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.reset_index(drop=True,inplace= True)\n",
    "test_dataset.reset_index(drop=True,inplace= True)\n",
    "val_dataset.reset_index(drop=True,inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_mapping = {'INFORMATION-TECHNOLOGY': 0,\\\n",
    "                 'ENGINEERING':1, \\\n",
    "                 'BUSINESS-DEVELOPMENT':2, \\\n",
    "                 'SALES':3,\\\n",
    "                 'HR':4, \\\n",
    "                 'FITNESS': 5 , \\\n",
    "                 'ARTS':6,\\\n",
    "                 'ADVOCATE':7,\\\n",
    "                 'CONSTRUCTION':8,\\\n",
    "                 'AVIATION':9,\\\n",
    "                 'FINANCE':10,\\\n",
    "                 'CHEF':11,\\\n",
    "                 'ACCOUNTANT':12,\\\n",
    "                 'BANKING':13,\\\n",
    "                 'HEALTHCARE':14,\\\n",
    "                 'CONSULTANT':15,\\\n",
    "                 'PUBLIC-RELATIONS':16,\\\n",
    "                 'DESIGNER':17, \\\n",
    "                 'TEACHER':18, \\\n",
    "                 'APPAREL':19, \\\n",
    "                 'DIGITAL-MEDIA':20,\\\n",
    "                 'AGRICULTURE':21, \\\n",
    "                 'AUTOMOBILE':22,\\\n",
    "                 'BPO':23\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(label):\n",
    "    return label_mapping[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[\"Category\"]= train_dataset[\"Category\"].apply(labeling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token = tokenizer.encode_plus(\n",
    "    train_dataset['Resume_clean'].iloc[0], \n",
    "    max_length=256, \n",
    "    truncation=True, \n",
    "    padding='max_length', \n",
    "    add_special_tokens=True,\n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 256), dtype=int32, numpy=\n",
       "array([[  101,  1900,  6213,  6678,  4160,   184,  8223, 10550,  4385,\n",
       "         7087,  2377,  3645,  1718,  1788,  2377,  3645,  1718,  1788,\n",
       "         3694, 11529,  1700,  2383,  1543,  4812, 11019,  9524,  2377,\n",
       "         3645, 17092,  1788,  1992, 11147,  1671,  3693,  8918,  2989,\n",
       "        25059,  6487,  5392,  2070,  1437,   180,  1643, 25247,   175,\n",
       "         1306,  6442,  3285,   193,  1306,   177,  1830, 10182,  3094,\n",
       "         2541,  1954,  1900,  6213,  6678,  1419,  1271,  1331,  1352,\n",
       "         3689, 10700,  1359,  3044,  2818,  2731,  2319, 10209, 25344,\n",
       "        11000,  3213,  1218,  4256,  1231,  5053,  7174,  7232,  3693,\n",
       "         2134,   189, 20144, 21143, 14561,  4973,  1788, 14561,  5035,\n",
       "         1844,  2057, 16878,  5494,  2134,  4422,  6298,  6678,  2344,\n",
       "         9342,  2755,  1470,  4125,  3181,  6213,  6678,  2338,  1470,\n",
       "         2029,  4291, 15187, 19396,  3881,  5911,  3294,  1934,  2394,\n",
       "         2561,  2344,  8826,  4731,  5494,  3232,  6802, 13681, 19916,\n",
       "        13681, 10407,  1856,  9630,  8826,  3689,  2344,  7380,  8132,\n",
       "         4125,  4739,  8525,  4868,  2134, 13372,  2313,  5845,  3914,\n",
       "         5845,  3511,  3675,  4226, 19961,  1701, 14445,  5218,  1696,\n",
       "        26027,  8616,  2134,  2194,  3645, 26789,  6213,  6678,  7061,\n",
       "         3693,  1470,  4125,  2619,  2546,  4570,  1788,  9131,  1419,\n",
       "         1271,  1331,  1352,  1661,  2670,  1718, 14561,  4973,  1788,\n",
       "        11019,  9524,  1687,  2194,  7995,  9496,  7550,  1298,  4301,\n",
       "         5052, 10940,  1671, 10700,  1872,  6228,  7623,  9403,  5877,\n",
       "         2519,  4010,  3703,  2194,  3772,  2365,  1972,  4607,  2530,\n",
       "         1690, 13692, 26027,  2136,  6213, 11717,   174,  2394,  7481,\n",
       "         2070,  1778,  2648,  1218,  1934,  2394,  7995,  3294,  5873,\n",
       "        14199,  2546,  1651,  3558,  8815, 11757,  6607,  6213,  8132,\n",
       "         1555,  1671,  2500,  1848,  1718,  5127,  9991,  1419,  1271,\n",
       "        18881,  5793,  3870,  2993,  3622,  4959,  1933,  9668,  9410,\n",
       "         2463,  7995,  2817,   102]], dtype=int32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "token.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input_ids = np.zeros((len(train_dataset), 256))\n",
    "X_attn_masks = np.zeros((len(train_dataset), 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(df, ids, masks, tokenizer):\n",
    "    for i, text in tqdm(enumerate(df['Resume_clean'])):\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=256, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            add_special_tokens=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        ids[i, :] = tokenized_text.input_ids\n",
    "        masks[i, :] = tokenized_text.attention_mask\n",
    "    return ids, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2249it [00:08, 266.31it/s]\n"
     ]
    }
   ],
   "source": [
    "X_input_ids, X_attn_masks = generate_training_data(train_dataset, X_input_ids, X_attn_masks, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2249, 24)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.zeros((len(train_dataset), 24))\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[np.arange(len(train_dataset)), train_dataset['Category'].values] = 1 # one-hot encoded target tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(24,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a data pipeline using tensorflow dataset utility, creates batches of data for easy loading...\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n",
    "dataset.take(1) # one sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attn_masks\n",
    "    }, labels\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(SentimentDatasetMapFunction) # converting to required format for tensorflow dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(24,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(10000).batch(16, drop_remainder=True) # batch size, drop any left out tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.8\n",
    "train_size = int((len(train_dataset)//16)*p) # for each 16 batch of data we will have len(df)//16 samples, take 80% of that for train.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tf_model.h5: 100%|██████████| 527M/527M [01:03<00:00, 8.24MB/s] \n",
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained('bert-base-cased') # bert base model with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 256,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " intermediate_layer (Dense)     (None, 512)          393728      ['bert[1][1]']                   \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 24)           12312       ['intermediate_layer[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,716,312\n",
      "Trainable params: 108,716,312\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defining 2 input layers for input_ids and attn_masks\n",
    "input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\n",
    "attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n",
    "\n",
    "bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\n",
    "intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\n",
    "output_layer = tf.keras.layers.Dense(24, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n",
    "\n",
    "sentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model.compile(optimizer=optim, loss=loss_func, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "112/112 [==============================] - ETA: 0s - loss: 2.5925 - accuracy: 0.3382"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 15:53:03.589210: E tensorflow/core/framework/node_def_util.cc:675] NodeDef mentions attribute epsilon which is not in the op definition: Op<name=_MklFusedBatchMatMulV2; signature=x:T, y:T, args:num_args*T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]; attr=adj_x:bool,default=false; attr=adj_y:bool,default=false; attr=num_args:int,min=0; attr=fused_ops:list(string),default=[]> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node model_1/bert/encoder/layer_._0/attention/self/ArithmeticOptimizer/AddOpsRewrite_add_1}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 292s 3s/step - loss: 2.5925 - accuracy: 0.3382 - val_loss: 2.0106 - val_accuracy: 0.5536\n",
      "Epoch 2/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 1.5913 - accuracy: 0.6847 - val_loss: 1.0422 - val_accuracy: 0.7879\n",
      "Epoch 3/50\n",
      "112/112 [==============================] - 293s 3s/step - loss: 1.0164 - accuracy: 0.7946 - val_loss: 0.7111 - val_accuracy: 0.8638\n",
      "Epoch 4/50\n",
      "112/112 [==============================] - 293s 3s/step - loss: 0.7626 - accuracy: 0.8449 - val_loss: 0.5454 - val_accuracy: 0.8705\n",
      "Epoch 5/50\n",
      "112/112 [==============================] - 293s 3s/step - loss: 0.5739 - accuracy: 0.8800 - val_loss: 0.3927 - val_accuracy: 0.9241\n",
      "Epoch 6/50\n",
      "112/112 [==============================] - 293s 3s/step - loss: 0.5115 - accuracy: 0.8867 - val_loss: 0.4083 - val_accuracy: 0.9040\n",
      "Epoch 7/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.4373 - accuracy: 0.9023 - val_loss: 0.3116 - val_accuracy: 0.9241\n",
      "Epoch 8/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.3785 - accuracy: 0.9062 - val_loss: 0.2375 - val_accuracy: 0.9509\n",
      "Epoch 9/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.3243 - accuracy: 0.9224 - val_loss: 0.2323 - val_accuracy: 0.9353\n",
      "Epoch 10/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.2923 - accuracy: 0.9269 - val_loss: 0.1843 - val_accuracy: 0.9531\n",
      "Epoch 11/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.2734 - accuracy: 0.9353 - val_loss: 0.1754 - val_accuracy: 0.9598\n",
      "Epoch 12/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.2183 - accuracy: 0.9481 - val_loss: 0.1113 - val_accuracy: 0.9754\n",
      "Epoch 13/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.1854 - accuracy: 0.9570 - val_loss: 0.1106 - val_accuracy: 0.9799\n",
      "Epoch 14/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.1540 - accuracy: 0.9665 - val_loss: 0.0806 - val_accuracy: 0.9799\n",
      "Epoch 15/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.1368 - accuracy: 0.9704 - val_loss: 0.0728 - val_accuracy: 0.9866\n",
      "Epoch 16/50\n",
      "112/112 [==============================] - 293s 3s/step - loss: 0.1180 - accuracy: 0.9794 - val_loss: 0.0621 - val_accuracy: 0.9866\n",
      "Epoch 17/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0960 - accuracy: 0.9827 - val_loss: 0.0392 - val_accuracy: 0.9978\n",
      "Epoch 18/50\n",
      "112/112 [==============================] - 293s 3s/step - loss: 0.0907 - accuracy: 0.9827 - val_loss: 0.0566 - val_accuracy: 0.9911\n",
      "Epoch 19/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0811 - accuracy: 0.9827 - val_loss: 0.0423 - val_accuracy: 0.9933\n",
      "Epoch 20/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0607 - accuracy: 0.9894 - val_loss: 0.0197 - val_accuracy: 0.9978\n",
      "Epoch 21/50\n",
      "112/112 [==============================] - 303s 3s/step - loss: 0.0509 - accuracy: 0.9933 - val_loss: 0.0213 - val_accuracy: 0.9978\n",
      "Epoch 22/50\n",
      "112/112 [==============================] - 326s 3s/step - loss: 0.0412 - accuracy: 0.9950 - val_loss: 0.0232 - val_accuracy: 0.9955\n",
      "Epoch 23/50\n",
      "112/112 [==============================] - 302s 3s/step - loss: 0.0357 - accuracy: 0.9944 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "112/112 [==============================] - 296s 3s/step - loss: 0.0340 - accuracy: 0.9955 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0287 - accuracy: 0.9955 - val_loss: 0.0192 - val_accuracy: 0.9978\n",
      "Epoch 26/50\n",
      "112/112 [==============================] - 293s 3s/step - loss: 0.0313 - accuracy: 0.9961 - val_loss: 0.0129 - val_accuracy: 0.9978\n",
      "Epoch 27/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0208 - accuracy: 0.9978 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "112/112 [==============================] - 293s 3s/step - loss: 0.0330 - accuracy: 0.9950 - val_loss: 0.0626 - val_accuracy: 0.9777\n",
      "Epoch 29/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0411 - accuracy: 0.9939 - val_loss: 0.0286 - val_accuracy: 0.9933\n",
      "Epoch 30/50\n",
      "112/112 [==============================] - 293s 3s/step - loss: 0.0282 - accuracy: 0.9955 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0224 - accuracy: 0.9961 - val_loss: 0.0059 - val_accuracy: 0.9978\n",
      "Epoch 32/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0146 - accuracy: 0.9978 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "112/112 [==============================] - 297s 3s/step - loss: 0.0219 - accuracy: 0.9961 - val_loss: 0.0194 - val_accuracy: 0.9978\n",
      "Epoch 34/50\n",
      "112/112 [==============================] - 296s 3s/step - loss: 0.0139 - accuracy: 0.9972 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "112/112 [==============================] - 293s 3s/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0069 - accuracy: 0.9994 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "112/112 [==============================] - 293s 3s/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0155 - accuracy: 0.9972 - val_loss: 0.0245 - val_accuracy: 0.9955\n",
      "Epoch 40/50\n",
      "112/112 [==============================] - 293s 3s/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.0068 - val_accuracy: 0.9978\n",
      "Epoch 41/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0300 - accuracy: 0.9927 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "112/112 [==============================] - 293s 3s/step - loss: 0.0284 - accuracy: 0.9961 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0278 - accuracy: 0.9944 - val_loss: 0.0200 - val_accuracy: 0.9933\n",
      "Epoch 44/50\n",
      "112/112 [==============================] - 293s 3s/step - loss: 0.0171 - accuracy: 0.9950 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0179 - accuracy: 0.9961 - val_loss: 0.0082 - val_accuracy: 0.9955\n",
      "Epoch 46/50\n",
      "112/112 [==============================] - 293s 3s/step - loss: 0.0169 - accuracy: 0.9955 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.0089 - val_accuracy: 0.9978\n",
      "Epoch 48/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0238 - accuracy: 0.9944 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "112/112 [==============================] - 294s 3s/step - loss: 0.0138 - accuracy: 0.9961 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "112/112 [==============================] - 295s 3s/step - loss: 0.0138 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "hist = sentiment_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "sentiment_model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
