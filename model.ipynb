{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourav/miniconda3/envs/rc/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_mapping = {'INFORMATION-TECHNOLOGY': 0,\\\n",
    "                 'ENGINEERING':1, \\\n",
    "                 'BUSINESS-DEVELOPMENT':2, \\\n",
    "                 'SALES':3,\\\n",
    "                 'HR':4, \\\n",
    "                 'FITNESS': 5 , \\\n",
    "                 'ARTS':6,\\\n",
    "                 'ADVOCATE':7,\\\n",
    "                 'CONSTRUCTION':8,\\\n",
    "                 'AVIATION':9,\\\n",
    "                 'FINANCE':10,\\\n",
    "                 'CHEF':11,\\\n",
    "                 'ACCOUNTANT':12,\\\n",
    "                 'BANKING':13,\\\n",
    "                 'HEALTHCARE':14,\\\n",
    "                 'CONSULTANT':15,\\\n",
    "                 'PUBLIC-RELATIONS':16,\\\n",
    "                 'DESIGNER':17, \\\n",
    "                 'TEACHER':18, \\\n",
    "                 'APPAREL':19, \\\n",
    "                 'DIGITAL-MEDIA':20,\\\n",
    "                 'AGRICULTURE':21, \\\n",
    "                 'AUTOMOBILE':22,\\\n",
    "                 'BPO':23\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(label):\n",
    "    return label_mapping[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/dataset/train_aug.csv\")\n",
    "val_data = pd.read_csv(\"data/dataset/val.csv\")\n",
    "test_data = pd.read_csv(\"data/dataset/test.csv\")\n",
    "\n",
    "\n",
    "train_data.dropna(inplace= True)\n",
    "val_data.dropna(inplace= True)\n",
    "test_data.dropna(inplace= True)\n",
    "\n",
    "train_data[\"Category\"] = train_data[\"Category\"].apply(labeling)\n",
    "test_data[\"Category\"] = test_data[\"Category\"].apply(labeling)\n",
    "val_data[\"Category\"] = val_data[\"Category\"].apply(labeling) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 522kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 6.27kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 302kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 436M/436M [00:38<00:00, 11.2MB/s] \n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloader(df, tokenizer, max_length , batch_size):\n",
    "    ids = np.zeros((len(df), max_length))\n",
    "    masks = np.zeros((len(df), max_length))\n",
    "    Y_labels = df['Category'].values\n",
    "\n",
    "    for i, text in tqdm(enumerate(df['Resume_clean'])):\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=max_length, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        ids[i, :] = tokenized_text.input_ids\n",
    "        masks[i, :] = tokenized_text.attention_mask\n",
    "    \n",
    "    X_ids =  torch.tensor(ids, dtype=torch.long)\n",
    "    X_masks = torch.tensor(masks, dtype=torch.long)\n",
    "    Y_labels = torch.tensor(Y_labels, dtype=torch.long)\n",
    "    dataset = TensorDataset(X_ids, X_masks, Y_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3599it [00:13, 263.33it/s]\n",
      "281it [00:01, 266.78it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader = generate_dataloader(train_data, tokenizer,max_length =256,batch_size = 64)\n",
    "val_loader = generate_dataloader(val_data, tokenizer,max_length =256,batch_size = 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the model architecture\n",
    "# class TextModel(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(TextModel, self).__init__()\n",
    "#         self.bert = model\n",
    "#         self.intermediate_layer = nn.Linear(768, 512)\n",
    "#         self.output_layer = nn.Linear(512, num_classes)\n",
    "        \n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert(input_ids, attention_mask=attention_mask)[1]\n",
    "#         intermediate = self.intermediate_layer(outputs)\n",
    "#         logits = self.output_layer(intermediate)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the model architecture with dropout and L2 regularization\n",
    "# class TextModel(nn.Module):\n",
    "#     def __init__(self, num_classes, dropout_prob=0.3, l2_reg=1e-5):\n",
    "#         super(TextModel, self).__init__()\n",
    "#         self.bert = model\n",
    "#         self.intermediate_layer = nn.Linear(768, 512)\n",
    "#         self.dropout = nn.Dropout(dropout_prob)  # Dropout layer added\n",
    "#         self.output_layer = nn.Linear(512, num_classes)\n",
    "        \n",
    "#         # L2 regularization added to linear layers\n",
    "#         self.intermediate_layer.weight.data = nn.init.kaiming_normal_(self.intermediate_layer.weight.data)\n",
    "#         self.intermediate_layer.bias.data.fill_(0)\n",
    "#         self.output_layer.weight.data = nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "#         self.output_layer.bias.data.fill_(0)\n",
    "        \n",
    "#         self.l2_reg = l2_reg\n",
    "    \n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert(input_ids, attention_mask=attention_mask)[1]\n",
    "#         intermediate = self.intermediate_layer(outputs)\n",
    "#         intermediate = self.dropout(intermediate)  # Apply dropout\n",
    "#         logits = self.output_layer(intermediate)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_prob=0.3, l2_reg=1e-5, l1_reg=1e-5):\n",
    "        super(TextModel, self).__init__()\n",
    "        self.bert = model\n",
    "        self.intermediate_layer = nn.Linear(768, 512)\n",
    "        self.dropout = nn.Dropout(dropout_prob)  # Dropout layer added\n",
    "        self.output_layer = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # L2 regularization added to linear layers\n",
    "        self.intermediate_layer.weight.data = nn.init.kaiming_normal_(self.intermediate_layer.weight.data)\n",
    "        self.intermediate_layer.bias.data.fill_(0)\n",
    "        self.output_layer.weight.data = nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "        self.output_layer.bias.data.fill_(0)\n",
    "        \n",
    "        self.l2_reg = l2_reg\n",
    "        self.l1_reg = l1_reg\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)[1]\n",
    "        intermediate = self.intermediate_layer(outputs)\n",
    "        intermediate = self.dropout(intermediate)  # Apply dropout\n",
    "        logits = self.output_layer(intermediate)\n",
    "        return logits\n",
    "\n",
    "    def l1_loss(self):\n",
    "        l1_loss = torch.tensor(0.).to(device)\n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:  # Only apply L1 regularization to linear layers\n",
    "                l1_loss += torch.norm(param, p=1)  # L1 norm\n",
    "        return self.l1_reg * l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = 24\n",
    "# dropout_prob = 0.3  # Adjust the dropout probability as needed\n",
    "# l2_reg = 1e-5  # Adjust the regularization strength as needed\n",
    "# model = TextModel(num_classes, dropout_prob=dropout_prob, l2_reg=l2_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with dropout, L2, and L1 regularization\n",
    "num_classes = 24\n",
    "dropout_prob = 0.3  # Adjust the dropout probability as needed\n",
    "l2_reg = 1e-5  # Adjust the L2 regularization strength as needed\n",
    "l1_reg = 1e-5  # Adjust the L1 regularization strength as needed\n",
    "model = TextModel(num_classes, dropout_prob=dropout_prob, l2_reg=l2_reg, l1_reg=l1_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (intermediate_layer): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (output_layer): Linear(in_features=512, out_features=24, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of training steps\n",
    "num_epochs = 25\n",
    "num_train_steps = len(train_loader) * num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "Y_labels = train_data['Category'].values\n",
    "class_weights = torch.tensor([1.0 / count for count in np.bincount(Y_labels)], dtype=torch.float)\n",
    "class_weights = class_weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights) \n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define early stopping and model checkpointing\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# train_acc =[]\n",
    "# valid_acc = []\n",
    "# train_loss =[]\n",
    "\n",
    "\n",
    "# best_val_accuracy = 0.0\n",
    "# early_stopping_counter = 0\n",
    "\n",
    "# # Training loop with early stopping and model checkpointing\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     predictions = []\n",
    "#     targets = []\n",
    "#     print(f\"---------Epoch:{epoch}----------\")\n",
    "#     for batch in tqdm(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         input_ids, attn_masks, labels = batch\n",
    "#         input_ids, attn_masks, labels = input_ids.to(device), attn_masks.to(device), labels.to(device)\n",
    "        \n",
    "#         outputs = model(input_ids, attn_masks)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         predictions.extend(torch.argmax(outputs, dim=1).tolist())\n",
    "#         targets.extend(labels.tolist())\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     acc = accuracy_score(targets, predictions)\n",
    "\n",
    "#     # Validation step\n",
    "#     model.eval()\n",
    "#     val_predictions = []\n",
    "#     val_targets = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for val_batch in tqdm(val_loader):\n",
    "#             val_input_ids, val_attn_masks, val_labels = val_batch\n",
    "#             val_input_ids, val_attn_masks, val_labels = val_input_ids.to(device), val_attn_masks.to(device), val_labels.to(device)\n",
    "            \n",
    "#             val_outputs = model(val_input_ids, val_attn_masks)\n",
    "#             val_predictions.extend(torch.argmax(val_outputs, dim=1).tolist())\n",
    "#             val_targets.extend(val_labels.tolist())\n",
    "\n",
    "#     val_acc = accuracy_score(val_targets, val_predictions)\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Avg. Loss: {avg_loss:.4f} - Accuracy: {acc:.4f} - Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "#     # Check for early stopping\n",
    "#     if acc > best_val_accuracy:\n",
    "#         best_val_accuracy = acc\n",
    "#         early_stopping_counter = 0\n",
    "#         torch.save(model.state_dict(), f\"model_ckpt/best_model_epoch_{epoch+1}_{val_acc}.pt\")\n",
    "#     else:\n",
    "#         early_stopping_counter += 1\n",
    "#         if early_stopping_counter >= 8:\n",
    "#             print(\"Early stopping triggered.\")\n",
    "#             break\n",
    "#     train_acc.append(acc)\n",
    "#     train_loss.append(avg_loss)\n",
    "#     valid_acc.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define early stopping and model checkpointing\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# train_acc =[]\n",
    "# valid_acc = []\n",
    "# train_loss =[]\n",
    "\n",
    "\n",
    "# best_val_accuracy = 0.0\n",
    "# early_stopping_counter = 0\n",
    "\n",
    "# # Training loop with early stopping and model checkpointing\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     predictions = []\n",
    "#     targets = []\n",
    "#     print(f\"---------Epoch:{epoch}----------\")\n",
    "#     for batch in tqdm(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         input_ids, attn_masks, labels = batch\n",
    "#         input_ids, attn_masks, labels = input_ids.to(device), attn_masks.to(device), labels.to(device)\n",
    "        \n",
    "#         outputs = model(input_ids, attn_masks)\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         # Apply L2 regularization to linear layers\n",
    "#         l2_loss = torch.tensor(0.).to(device)\n",
    "#         for param in model.parameters():\n",
    "#             if param.dim() > 1:  # Only apply regularization to linear layers\n",
    "#                 l2_loss += torch.norm(param, p=2)  # L2 norm\n",
    "#         loss += l2_reg * l2_loss\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         predictions.extend(torch.argmax(outputs, dim=1).tolist())\n",
    "#         targets.extend(labels.tolist())\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     acc = accuracy_score(targets, predictions)\n",
    "\n",
    "#     # Validation step\n",
    "#     model.eval()\n",
    "#     val_predictions = []\n",
    "#     val_targets = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for val_batch in tqdm(val_loader):\n",
    "#             val_input_ids, val_attn_masks, val_labels = val_batch\n",
    "#             val_input_ids, val_attn_masks, val_labels = val_input_ids.to(device), val_attn_masks.to(device), val_labels.to(device)\n",
    "            \n",
    "#             val_outputs = model(val_input_ids, val_attn_masks)\n",
    "#             val_predictions.extend(torch.argmax(val_outputs, dim=1).tolist())\n",
    "#             val_targets.extend(val_labels.tolist())\n",
    "\n",
    "#     val_acc = accuracy_score(val_targets, val_predictions)\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Avg. Loss: {avg_loss:.4f} - Accuracy: {acc:.4f} - Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "#     # Check for early stopping\n",
    "#     if acc > best_val_accuracy:\n",
    "#         best_val_accuracy = acc\n",
    "#         early_stopping_counter = 0\n",
    "#         torch.save(model.state_dict(), f\"model_ckpt/best_model_epoch_{epoch+1}_{val_acc}.pt\")\n",
    "#     else:\n",
    "#         early_stopping_counter += 1\n",
    "#         if early_stopping_counter >= 8:\n",
    "#             print(\"Early stopping triggered.\")\n",
    "#             break\n",
    "#     train_acc.append(acc)\n",
    "#     train_loss.append(avg_loss)\n",
    "#     valid_acc.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Epoch:0----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:13<00:00,  1.29s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Avg. Loss: 35.0581 - Accuracy: 0.2359 - Val Accuracy: 0.5160\n",
      "---------Epoch:1----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:16<00:00,  1.35s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 - Avg. Loss: 33.9342 - Accuracy: 0.5565 - Val Accuracy: 0.7722\n",
      "---------Epoch:2----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:18<00:00,  1.37s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 - Avg. Loss: 33.1055 - Accuracy: 0.7610 - Val Accuracy: 0.8256\n",
      "---------Epoch:3----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:18<00:00,  1.37s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 - Avg. Loss: 32.6703 - Accuracy: 0.8552 - Val Accuracy: 0.8327\n",
      "---------Epoch:4----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:18<00:00,  1.37s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 - Avg. Loss: 32.4117 - Accuracy: 0.8916 - Val Accuracy: 0.8399\n",
      "---------Epoch:5----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:18<00:00,  1.37s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 - Avg. Loss: 32.1964 - Accuracy: 0.9255 - Val Accuracy: 0.8292\n",
      "---------Epoch:6----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:18<00:00,  1.37s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 - Avg. Loss: 32.0316 - Accuracy: 0.9383 - Val Accuracy: 0.8505\n",
      "---------Epoch:7----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:18<00:00,  1.37s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 - Avg. Loss: 31.8840 - Accuracy: 0.9514 - Val Accuracy: 0.8256\n",
      "---------Epoch:8----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:18<00:00,  1.37s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 - Avg. Loss: 31.7448 - Accuracy: 0.9650 - Val Accuracy: 0.8256\n",
      "---------Epoch:9----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:18<00:00,  1.37s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 - Avg. Loss: 31.6303 - Accuracy: 0.9686 - Val Accuracy: 0.8292\n",
      "---------Epoch:10----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:17<00:00,  1.37s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 - Avg. Loss: 31.4964 - Accuracy: 0.9833 - Val Accuracy: 0.8434\n",
      "---------Epoch:11----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:17<00:00,  1.36s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 - Avg. Loss: 31.3941 - Accuracy: 0.9797 - Val Accuracy: 0.8363\n",
      "---------Epoch:12----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:17<00:00,  1.37s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 - Avg. Loss: 31.2903 - Accuracy: 0.9825 - Val Accuracy: 0.8470\n",
      "---------Epoch:13----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:17<00:00,  1.36s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 - Avg. Loss: 31.1761 - Accuracy: 0.9889 - Val Accuracy: 0.8292\n",
      "---------Epoch:14----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:17<00:00,  1.36s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 - Avg. Loss: 31.0732 - Accuracy: 0.9894 - Val Accuracy: 0.8363\n",
      "---------Epoch:15----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:17<00:00,  1.37s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 - Avg. Loss: 30.9762 - Accuracy: 0.9933 - Val Accuracy: 0.8327\n",
      "---------Epoch:16----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:17<00:00,  1.37s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 - Avg. Loss: 30.8747 - Accuracy: 0.9950 - Val Accuracy: 0.8221\n",
      "---------Epoch:17----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:17<00:00,  1.36s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 - Avg. Loss: 30.7835 - Accuracy: 0.9928 - Val Accuracy: 0.8363\n",
      "---------Epoch:18----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:17<00:00,  1.37s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 - Avg. Loss: 30.6848 - Accuracy: 0.9950 - Val Accuracy: 0.8292\n",
      "---------Epoch:19----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:17<00:00,  1.37s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 - Avg. Loss: 30.5870 - Accuracy: 0.9972 - Val Accuracy: 0.8327\n",
      "---------Epoch:20----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:17<00:00,  1.36s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 - Avg. Loss: 30.4960 - Accuracy: 0.9958 - Val Accuracy: 0.8363\n",
      "---------Epoch:21----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:17<00:00,  1.36s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 - Avg. Loss: 30.3985 - Accuracy: 0.9975 - Val Accuracy: 0.8399\n",
      "---------Epoch:22----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:17<00:00,  1.37s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 - Avg. Loss: 30.3060 - Accuracy: 0.9983 - Val Accuracy: 0.8327\n",
      "---------Epoch:23----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:17<00:00,  1.36s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25 - Avg. Loss: 30.2096 - Accuracy: 0.9992 - Val Accuracy: 0.8399\n",
      "---------Epoch:24----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [01:17<00:00,  1.36s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 - Avg. Loss: 30.1174 - Accuracy: 0.9978 - Val Accuracy: 0.8292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define early stopping and model checkpointing\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_acc =[]\n",
    "valid_acc = []\n",
    "train_loss =[]\n",
    "\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Training loop with early stopping and model checkpointing\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    print(f\"---------Epoch:{epoch}----------\")\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attn_masks, labels = batch\n",
    "        input_ids, attn_masks, labels = input_ids.to(device), attn_masks.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attn_masks)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Apply L2 and L1 regularization\n",
    "        l2_loss = torch.tensor(0.).to(device)\n",
    "        l1_loss = model.l1_loss()\n",
    "        for param in model.parameters():\n",
    "            if param.dim() > 1:\n",
    "                l2_loss += torch.norm(param, p=2)\n",
    "        loss += l2_reg * l2_loss + l1_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predictions.extend(torch.argmax(outputs, dim=1).tolist())\n",
    "        targets.extend(labels.tolist())\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    acc = accuracy_score(targets, predictions)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(val_loader):\n",
    "            val_input_ids, val_attn_masks, val_labels = val_batch\n",
    "            val_input_ids, val_attn_masks, val_labels = val_input_ids.to(device), val_attn_masks.to(device), val_labels.to(device)\n",
    "            \n",
    "            val_outputs = model(val_input_ids, val_attn_masks)\n",
    "            val_predictions.extend(torch.argmax(val_outputs, dim=1).tolist())\n",
    "            val_targets.extend(val_labels.tolist())\n",
    "\n",
    "    val_acc = accuracy_score(val_targets, val_predictions)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Avg. Loss: {avg_loss:.4f} - Accuracy: {acc:.4f} - Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if acc > best_val_accuracy:\n",
    "        best_val_accuracy = acc\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), f\"model_ckpt/best_model_epoch_{epoch+1}_{val_acc}.pt\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= 8:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    train_acc.append(acc)\n",
    "    train_loss.append(avg_loss)\n",
    "    valid_acc.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
