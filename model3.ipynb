{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(label):\n",
    "    return label_mapping[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_mapping = {'INFORMATION-TECHNOLOGY': 0,\\\n",
    "                 'ENGINEERING':1, \\\n",
    "                 'BUSINESS-DEVELOPMENT':2, \\\n",
    "                 'SALES':3,\\\n",
    "                 'HR':4, \\\n",
    "                 'FITNESS': 5 , \\\n",
    "                 'ARTS':6,\\\n",
    "                 'ADVOCATE':7,\\\n",
    "                 'CONSTRUCTION':8,\\\n",
    "                 'AVIATION':9,\\\n",
    "                 'FINANCE':10,\\\n",
    "                 'CHEF':11,\\\n",
    "                 'ACCOUNTANT':12,\\\n",
    "                 'BANKING':13,\\\n",
    "                 'HEALTHCARE':14,\\\n",
    "                 'CONSULTANT':15,\\\n",
    "                 'PUBLIC-RELATIONS':16,\\\n",
    "                 'DESIGNER':17, \\\n",
    "                 'TEACHER':18, \\\n",
    "                 'APPAREL':19, \\\n",
    "                 'DIGITAL-MEDIA':20,\\\n",
    "                 'AGRICULTURE':21, \\\n",
    "                 'AUTOMOBILE':22,\\\n",
    "                 'BPO':23\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/dataset/train.csv\")\n",
    "val_data = pd.read_csv(\"data/dataset/val.csv\")\n",
    "test_data = pd.read_csv(\"data/dataset/test.csv\")\n",
    "\n",
    "\n",
    "train_data.dropna(inplace= True)\n",
    "val_data.dropna(inplace= True)\n",
    "test_data.dropna(inplace= True)\n",
    "\n",
    "train_data[\"Category\"] = train_data[\"Category\"].apply(labeling)\n",
    "test_data[\"Category\"] = test_data[\"Category\"].apply(labeling)\n",
    "val_data[\"Category\"] = val_data[\"Category\"].apply(labeling) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloader(df, tokenizer, max_length , batch_size):\n",
    "    ids = np.zeros((len(df), max_length))\n",
    "    masks = np.zeros((len(df), max_length))\n",
    "    Y_labels = df['Category'].values\n",
    "\n",
    "    for i, text in tqdm(enumerate(df['Resume_clean'])):\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=max_length, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        ids[i, :] = tokenized_text.input_ids\n",
    "        masks[i, :] = tokenized_text.attention_mask\n",
    "    \n",
    "    X_ids =  torch.tensor(ids, dtype=torch.long)\n",
    "    X_masks = torch.tensor(masks, dtype=torch.long)\n",
    "    Y_labels = torch.tensor(Y_labels, dtype=torch.long)\n",
    "    dataset = TensorDataset(X_ids, X_masks, Y_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2249it [00:08, 265.97it/s]\n",
      "281it [00:01, 271.13it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader = generate_dataloader(train_data, tokenizer,max_length =256,batch_size = 64)\n",
    "val_loader = generate_dataloader(val_data, tokenizer,max_length =256,batch_size = 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the model architecture\n",
    "# class TextModel(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(TextModel, self).__init__()\n",
    "#         self.bert = model\n",
    "#         self.intermediate_layer = nn.Linear(768, 512)\n",
    "#         self.output_layer = nn.Linear(512, num_classes)\n",
    "        \n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert(input_ids, attention_mask=attention_mask)[1]\n",
    "#         intermediate = self.intermediate_layer(outputs)\n",
    "#         logits = self.output_layer(intermediate)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the model architecture with dropout and L2 regularization\n",
    "# class TextModel(nn.Module):\n",
    "#     def __init__(self, num_classes, dropout_prob=0.3, l2_reg=1e-5):\n",
    "#         super(TextModel, self).__init__()\n",
    "#         self.bert = model\n",
    "#         self.intermediate_layer = nn.Linear(768, 512)\n",
    "#         self.dropout = nn.Dropout(dropout_prob)  # Dropout layer added\n",
    "#         self.output_layer = nn.Linear(512, num_classes)\n",
    "        \n",
    "#         # L2 regularization added to linear layers\n",
    "#         self.intermediate_layer.weight.data = nn.init.kaiming_normal_(self.intermediate_layer.weight.data)\n",
    "#         self.intermediate_layer.bias.data.fill_(0)\n",
    "#         self.output_layer.weight.data = nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "#         self.output_layer.bias.data.fill_(0)\n",
    "        \n",
    "#         self.l2_reg = l2_reg\n",
    "    \n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert(input_ids, attention_mask=attention_mask)[1]\n",
    "#         intermediate = self.intermediate_layer(outputs)\n",
    "#         intermediate = self.dropout(intermediate)  # Apply dropout\n",
    "#         logits = self.output_layer(intermediate)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_prob=0.3, l2_reg=1e-5, l1_reg=1e-5):\n",
    "        super(TextModel, self).__init__()\n",
    "        self.bert = model\n",
    "        self.intermediate_layer = nn.Linear(768, 512)\n",
    "        self.dropout = nn.Dropout(dropout_prob)  # Dropout layer added\n",
    "        self.output_layer = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # L2 regularization added to linear layers\n",
    "        self.intermediate_layer.weight.data = nn.init.kaiming_normal_(self.intermediate_layer.weight.data)\n",
    "        self.intermediate_layer.bias.data.fill_(0)\n",
    "        self.output_layer.weight.data = nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "        self.output_layer.bias.data.fill_(0)\n",
    "        \n",
    "        self.l2_reg = l2_reg\n",
    "        self.l1_reg = l1_reg\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)[1]\n",
    "        intermediate = self.intermediate_layer(outputs)\n",
    "        intermediate = self.dropout(intermediate)  # Apply dropout\n",
    "        logits = self.output_layer(intermediate)\n",
    "        return logits\n",
    "\n",
    "    def l1_loss(self):\n",
    "        l1_loss = torch.tensor(0.).to(device)\n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:  # Only apply L1 regularization to linear layers\n",
    "                l1_loss += torch.norm(param, p=1)  # L1 norm\n",
    "        return self.l1_reg * l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = 24\n",
    "# dropout_prob = 0.3  # Adjust the dropout probability as needed\n",
    "# l2_reg = 1e-5  # Adjust the regularization strength as needed\n",
    "# model = TextModel(num_classes, dropout_prob=dropout_prob, l2_reg=l2_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with dropout, L2, and L1 regularization\n",
    "num_classes = 24\n",
    "dropout_prob = 0.3  # Adjust the dropout probability as needed\n",
    "l2_reg = 1e-5  # Adjust the L2 regularization strength as needed\n",
    "l1_reg = 1e-5  # Adjust the L1 regularization strength as needed\n",
    "model = TextModel(num_classes, dropout_prob=dropout_prob, l2_reg=l2_reg, l1_reg=l1_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (intermediate_layer): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (output_layer): Linear(in_features=512, out_features=24, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of training steps\n",
    "num_epochs = 25\n",
    "num_train_steps = len(train_loader) * num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "Y_labels = train_data['Category'].values\n",
    "class_weights = torch.tensor([1.0 / count for count in np.bincount(Y_labels)], dtype=torch.float)\n",
    "class_weights = class_weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights) \n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define early stopping and model checkpointing\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# train_acc =[]\n",
    "# valid_acc = []\n",
    "# train_loss =[]\n",
    "\n",
    "\n",
    "# best_val_accuracy = 0.0\n",
    "# early_stopping_counter = 0\n",
    "\n",
    "# # Training loop with early stopping and model checkpointing\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     predictions = []\n",
    "#     targets = []\n",
    "#     print(f\"---------Epoch:{epoch}----------\")\n",
    "#     for batch in tqdm(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         input_ids, attn_masks, labels = batch\n",
    "#         input_ids, attn_masks, labels = input_ids.to(device), attn_masks.to(device), labels.to(device)\n",
    "        \n",
    "#         outputs = model(input_ids, attn_masks)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         predictions.extend(torch.argmax(outputs, dim=1).tolist())\n",
    "#         targets.extend(labels.tolist())\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     acc = accuracy_score(targets, predictions)\n",
    "\n",
    "#     # Validation step\n",
    "#     model.eval()\n",
    "#     val_predictions = []\n",
    "#     val_targets = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for val_batch in tqdm(val_loader):\n",
    "#             val_input_ids, val_attn_masks, val_labels = val_batch\n",
    "#             val_input_ids, val_attn_masks, val_labels = val_input_ids.to(device), val_attn_masks.to(device), val_labels.to(device)\n",
    "            \n",
    "#             val_outputs = model(val_input_ids, val_attn_masks)\n",
    "#             val_predictions.extend(torch.argmax(val_outputs, dim=1).tolist())\n",
    "#             val_targets.extend(val_labels.tolist())\n",
    "\n",
    "#     val_acc = accuracy_score(val_targets, val_predictions)\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Avg. Loss: {avg_loss:.4f} - Accuracy: {acc:.4f} - Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "#     # Check for early stopping\n",
    "#     if acc > best_val_accuracy:\n",
    "#         best_val_accuracy = acc\n",
    "#         early_stopping_counter = 0\n",
    "#         torch.save(model.state_dict(), f\"model_ckpt/best_model_epoch_{epoch+1}_{val_acc}.pt\")\n",
    "#     else:\n",
    "#         early_stopping_counter += 1\n",
    "#         if early_stopping_counter >= 8:\n",
    "#             print(\"Early stopping triggered.\")\n",
    "#             break\n",
    "#     train_acc.append(acc)\n",
    "#     train_loss.append(avg_loss)\n",
    "#     valid_acc.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define early stopping and model checkpointing\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# train_acc =[]\n",
    "# valid_acc = []\n",
    "# train_loss =[]\n",
    "\n",
    "\n",
    "# best_val_accuracy = 0.0\n",
    "# early_stopping_counter = 0\n",
    "\n",
    "# # Training loop with early stopping and model checkpointing\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     predictions = []\n",
    "#     targets = []\n",
    "#     print(f\"---------Epoch:{epoch}----------\")\n",
    "#     for batch in tqdm(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         input_ids, attn_masks, labels = batch\n",
    "#         input_ids, attn_masks, labels = input_ids.to(device), attn_masks.to(device), labels.to(device)\n",
    "        \n",
    "#         outputs = model(input_ids, attn_masks)\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         # Apply L2 regularization to linear layers\n",
    "#         l2_loss = torch.tensor(0.).to(device)\n",
    "#         for param in model.parameters():\n",
    "#             if param.dim() > 1:  # Only apply regularization to linear layers\n",
    "#                 l2_loss += torch.norm(param, p=2)  # L2 norm\n",
    "#         loss += l2_reg * l2_loss\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         predictions.extend(torch.argmax(outputs, dim=1).tolist())\n",
    "#         targets.extend(labels.tolist())\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     acc = accuracy_score(targets, predictions)\n",
    "\n",
    "#     # Validation step\n",
    "#     model.eval()\n",
    "#     val_predictions = []\n",
    "#     val_targets = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for val_batch in tqdm(val_loader):\n",
    "#             val_input_ids, val_attn_masks, val_labels = val_batch\n",
    "#             val_input_ids, val_attn_masks, val_labels = val_input_ids.to(device), val_attn_masks.to(device), val_labels.to(device)\n",
    "            \n",
    "#             val_outputs = model(val_input_ids, val_attn_masks)\n",
    "#             val_predictions.extend(torch.argmax(val_outputs, dim=1).tolist())\n",
    "#             val_targets.extend(val_labels.tolist())\n",
    "\n",
    "#     val_acc = accuracy_score(val_targets, val_predictions)\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Avg. Loss: {avg_loss:.4f} - Accuracy: {acc:.4f} - Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "#     # Check for early stopping\n",
    "#     if acc > best_val_accuracy:\n",
    "#         best_val_accuracy = acc\n",
    "#         early_stopping_counter = 0\n",
    "#         torch.save(model.state_dict(), f\"model_ckpt/best_model_epoch_{epoch+1}_{val_acc}.pt\")\n",
    "#     else:\n",
    "#         early_stopping_counter += 1\n",
    "#         if early_stopping_counter >= 8:\n",
    "#             print(\"Early stopping triggered.\")\n",
    "#             break\n",
    "#     train_acc.append(acc)\n",
    "#     train_loss.append(avg_loss)\n",
    "#     valid_acc.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Epoch:0----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:37<00:00,  1.05s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Avg. Loss: 36.0068 - Accuracy: 0.0551 - Val Accuracy: 0.0854\n",
      "---------Epoch:1----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:38<00:00,  1.07s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 - Avg. Loss: 35.4055 - Accuracy: 0.1174 - Val Accuracy: 0.3310\n",
      "---------Epoch:2----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:38<00:00,  1.08s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 - Avg. Loss: 34.8306 - Accuracy: 0.3130 - Val Accuracy: 0.5587\n",
      "---------Epoch:3----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.09s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 - Avg. Loss: 34.1246 - Accuracy: 0.5389 - Val Accuracy: 0.6833\n",
      "---------Epoch:4----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.09s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 - Avg. Loss: 33.5369 - Accuracy: 0.6963 - Val Accuracy: 0.7972\n",
      "---------Epoch:5----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 - Avg. Loss: 33.0899 - Accuracy: 0.7946 - Val Accuracy: 0.7936\n",
      "---------Epoch:6----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 - Avg. Loss: 32.8475 - Accuracy: 0.8341 - Val Accuracy: 0.8149\n",
      "---------Epoch:7----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 - Avg. Loss: 32.6691 - Accuracy: 0.8604 - Val Accuracy: 0.8327\n",
      "---------Epoch:8----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 - Avg. Loss: 32.4802 - Accuracy: 0.8844 - Val Accuracy: 0.8256\n",
      "---------Epoch:9----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 - Avg. Loss: 32.3941 - Accuracy: 0.8915 - Val Accuracy: 0.8327\n",
      "---------Epoch:10----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 - Avg. Loss: 32.2425 - Accuracy: 0.9071 - Val Accuracy: 0.8292\n",
      "---------Epoch:11----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 - Avg. Loss: 32.1398 - Accuracy: 0.9186 - Val Accuracy: 0.8399\n",
      "---------Epoch:12----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 - Avg. Loss: 32.0577 - Accuracy: 0.9306 - Val Accuracy: 0.8327\n",
      "---------Epoch:13----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 - Avg. Loss: 31.9421 - Accuracy: 0.9386 - Val Accuracy: 0.8292\n",
      "---------Epoch:14----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 - Avg. Loss: 31.8689 - Accuracy: 0.9440 - Val Accuracy: 0.8363\n",
      "---------Epoch:15----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 - Avg. Loss: 31.7849 - Accuracy: 0.9515 - Val Accuracy: 0.8327\n",
      "---------Epoch:16----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 - Avg. Loss: 31.7083 - Accuracy: 0.9480 - Val Accuracy: 0.8363\n",
      "---------Epoch:17----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 - Avg. Loss: 31.6254 - Accuracy: 0.9586 - Val Accuracy: 0.8221\n",
      "---------Epoch:18----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 - Avg. Loss: 31.5468 - Accuracy: 0.9622 - Val Accuracy: 0.8363\n",
      "---------Epoch:19----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 - Avg. Loss: 31.4859 - Accuracy: 0.9609 - Val Accuracy: 0.8292\n",
      "---------Epoch:20----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 - Avg. Loss: 31.3983 - Accuracy: 0.9667 - Val Accuracy: 0.8399\n",
      "---------Epoch:21----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 - Avg. Loss: 31.3256 - Accuracy: 0.9715 - Val Accuracy: 0.8292\n",
      "---------Epoch:22----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 - Avg. Loss: 31.2631 - Accuracy: 0.9755 - Val Accuracy: 0.8292\n",
      "---------Epoch:23----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25 - Avg. Loss: 31.1940 - Accuracy: 0.9747 - Val Accuracy: 0.8221\n",
      "---------Epoch:24----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 - Avg. Loss: 31.1245 - Accuracy: 0.9791 - Val Accuracy: 0.8327\n"
     ]
    }
   ],
   "source": [
    "# Define early stopping and model checkpointing\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_acc =[]\n",
    "valid_acc = []\n",
    "train_loss =[]\n",
    "\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Training loop with early stopping and model checkpointing\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    print(f\"---------Epoch:{epoch}----------\")\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attn_masks, labels = batch\n",
    "        input_ids, attn_masks, labels = input_ids.to(device), attn_masks.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attn_masks)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Apply L2 and L1 regularization\n",
    "        l2_loss = torch.tensor(0.).to(device)\n",
    "        l1_loss = model.l1_loss()\n",
    "        for param in model.parameters():\n",
    "            if param.dim() > 1:\n",
    "                l2_loss += torch.norm(param, p=2)\n",
    "        loss += l2_reg * l2_loss + l1_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predictions.extend(torch.argmax(outputs, dim=1).tolist())\n",
    "        targets.extend(labels.tolist())\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    acc = accuracy_score(targets, predictions)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(val_loader):\n",
    "            val_input_ids, val_attn_masks, val_labels = val_batch\n",
    "            val_input_ids, val_attn_masks, val_labels = val_input_ids.to(device), val_attn_masks.to(device), val_labels.to(device)\n",
    "            \n",
    "            val_outputs = model(val_input_ids, val_attn_masks)\n",
    "            val_predictions.extend(torch.argmax(val_outputs, dim=1).tolist())\n",
    "            val_targets.extend(val_labels.tolist())\n",
    "\n",
    "    val_acc = accuracy_score(val_targets, val_predictions)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Avg. Loss: {avg_loss:.4f} - Accuracy: {acc:.4f} - Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if acc > best_val_accuracy:\n",
    "        best_val_accuracy = acc\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), f\"model_ckpt/best_model_epoch_{epoch+1}_{val_acc}.pt\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= 8:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    train_acc.append(acc)\n",
    "    train_loss.append(avg_loss)\n",
    "    valid_acc.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
